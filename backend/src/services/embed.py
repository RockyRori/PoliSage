import json
import os
from typing import Optional

import torch
from qdrant_client.http.models import Filter, FieldCondition, MatchValue
from qdrant_client.http.models import PointStruct, FilterSelector
from transformers import AutoTokenizer, AutoModel

from backend.config import qdrant, COLLECTION, UPLOAD_FOLDER


class BertLoader:
    """
    å°è£… Hugging Face çš„ BERT tokenizer å’Œ model åŠ è½½é€»è¾‘ã€‚
    å¦‚æœåˆå§‹åŒ–å¤±è´¥ï¼Œä¼šæŠ›å‡ºè¯¦ç»†é”™è¯¯ä¿¡æ¯ã€‚
    """

    def __init__(self, model_name: str = 'bert-base-multilingual-cased'):
        """
        åˆå§‹åŒ–å¹¶åŠ è½½ tokenizer å’Œ modelã€‚

        Args:
            model_name (str): é¢„è®­ç»ƒæ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º 'bert-base-multilingual-cased'ã€‚

        Raises:
            RuntimeError: å¦‚æœåŠ è½½å¤±è´¥ï¼ŒåŒ…å«å…·ä½“é”™è¯¯ä¿¡æ¯ã€‚
        """
        self.model_name = model_name
        self.tokenizer: Optional[AutoTokenizer] = None
        self.model: Optional[AutoModel] = None

        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True)
            self.model = AutoModel.from_pretrained(model_name, local_files_only=True)
        except Exception as e:
            raise RuntimeError(
                f"Failed to load tokenizer or model '{model_name}'. Error: {str(e)}"
            )

    @property
    def get_tokenizer(self):
        """è¿”å› tokenizerï¼Œå¦‚æœæœªåŠ è½½åˆ™æŠ›å‡ºé”™è¯¯ã€‚"""
        if self.tokenizer is None:
            raise RuntimeError("Tokenizer not loaded! Check initialization.")
        return self.tokenizer

    @property
    def get_model(self):
        """è¿”å› modelï¼Œå¦‚æœæœªåŠ è½½åˆ™æŠ›å‡ºé”™è¯¯ã€‚"""
        if self.model is None:
            raise RuntimeError("Model not loaded! Check initialization.")
        return self.model


# ç”Ÿæˆå®Œæ•´æ–‡ä»¶è·¯å¾„
def get_json_file_path(json_name):
    if not json_name.endswith(".json"):
        json_name += ".json"
    return os.path.join(UPLOAD_FOLDER, json_name)


# æ–‡æœ¬ç¼–ç å‡½æ•°
def encode_text(text):
    if not text or len(text.strip()) == 0:
        return None

    bert_loader = BertLoader('bert-base-multilingual-cased')
    inputs = bert_loader.get_tokenizer(text, return_tensors='pt', truncation=True, padding=True)
    with torch.no_grad():
        outputs = bert_loader.get_model(**inputs)
    return outputs.last_hidden_state[:, 0, :].squeeze().numpy().tolist()


# æ’å…¥å‘é‡åˆ° Qdrant
def insert_vectors(file_name):
    json_path = get_json_file_path(file_name)

    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    points = []
    unique_id = 1  # ç”Ÿæˆå”¯ä¸€ ID

    for item in data:
        item_type = item.get("type")
        page_idx = item.get("page_idx", -1)
        text = item.get("text", "")
        img_path = item.get("img_path", "")
        vector = encode_text(text)
        if vector is None:
            print(f"âš ï¸ è·³è¿‡ç©ºæ–‡æœ¬å—ï¼Œpage_idx={page_idx}")
            continue
        payload = {
            "file_name": file_name,
            "type": item_type,
            "page_idx": page_idx,
            "text": text,
            "img_path": img_path
        }
        points.append(PointStruct(id=unique_id, vector=vector, payload=payload))
        unique_id += 1

    if points:
        qdrant.upsert(
            collection_name=COLLECTION,
            points=points,
            wait=True
        )
        print(f"âœ… å·²æ’å…¥ {len(points)} ä¸ªå‘é‡ç‚¹ã€‚")
    else:
        print("âŒ æ²¡æœ‰å¯æ’å…¥çš„å‘é‡ã€‚")


def delete_vectors(file_name):
    """ä½¿ç”¨ FilterSelector æŒ‰ file_name åˆ é™¤æ‰€æœ‰ç‚¹ã€‚"""
    selector = FilterSelector(
        filter=Filter(
            must=[
                FieldCondition(
                    key="file_name",
                    match=MatchValue(value=file_name)
                )
            ]
        )
    )
    resp = qdrant.delete(
        collection_name=COLLECTION,
        points_selector=selector,
        wait=True
    )
    print(f"ğŸ—‘ï¸ å·²è°ƒç”¨ delete() åˆ é™¤ file_name={file_name} å¯¹åº”çš„å‘é‡ç‚¹ã€‚")


# ç¤ºä¾‹è¿è¡Œ
if __name__ == "__main__":
    json_name = "input"

    # æ’å…¥æ•°æ®
    insert_vectors(json_name)
    # åˆ é™¤æ•°æ®
    delete_vectors(json_name)
